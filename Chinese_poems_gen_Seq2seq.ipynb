{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D1r5gV29LLWJ"
   },
   "source": [
    "# Part II: Sequence to Sequence Models in tensorflow\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GwjBYH6hgPRP"
   },
   "source": [
    "## Task: Language Modeling\n",
    "Using tensorflow to build a simple seq2seq structure for language modeling.\n",
    "### Definition\n",
    "We input a sequence of words/characters to an RNN so that it can learn the probability distribution of the next word/character in the sequence given the history of previous characters. This will then allow us to generate text one unit at a time.\n",
    "\n",
    "We will use 全唐诗 as our training data, and try to generate new poems later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dnXFHqP7fmJw"
   },
   "source": [
    "## Check the content\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0bI7rsH2LU_L"
   },
   "source": [
    "The format of our input data is like this:\n",
    "\n",
    "`(optional title + \":\")poem`\n",
    "\n",
    "We will use only the poem part and not the title.\n",
    "\n",
    "However, some special cases like:\n",
    "\n",
    "* 河鱼未上冻，江蛰已闻雷。（见《纬略》）\n",
    "* □□□□□\n",
    "\n",
    "We need some preprocessing\n",
    "\n",
    "* Remove title\n",
    "* Remove spaces\n",
    "* Remove empty symbols\n",
    "* Replace other symbols\n",
    "\n",
    "Finally, we will randomize them. In case the model learning the same pattern and increase the variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HDxfxaNbE1Na"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "data_filename ='poetry.txt'\n",
    "poems = []\n",
    "with open(data_filename, \"r\") as in_file:\n",
    "    for line in in_file.readlines():\n",
    "        line = line.strip()\n",
    "        # find title if exists\n",
    "        if ':' in line:\n",
    "            line = line.split(':')\n",
    "        # some poems are empty\n",
    "        if len(line) == 2:\n",
    "            poem = line[1]\n",
    "        else: # only got title\n",
    "            continue\n",
    "        # discard if contains special symbols\n",
    "        if re.search(r'[(（《_□]', poem):\n",
    "            continue\n",
    "        # discard if too short or too long\n",
    "        if len(poem) < 5 or len(poem) > 40:\n",
    "            continue\n",
    "        # remove symbols\n",
    "        poem = re.sub(u'[，。]','',poem) # punctuation would appear many times, remove them in case model learned it.\n",
    "        poems.append(poem)\n",
    "\n",
    "poems = np.random.permutation(poems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11103"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(poems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mFjPmxWXlZJ2"
   },
   "source": [
    "We select 5 poems as our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tqbNdpZYO8ma"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11098, 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poems_train, poems_test = poems[:-5], poems[-5:]\n",
    "len(poems_train), len(poems_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sb9em1KyfuvZ"
   },
   "source": [
    "## Word to ID\n",
    "\n",
    "Use the tokenizer in Keras to tokenize the words. We set\n",
    "```python\n",
    "Tokenizer(num_words=None, lower=False, char_level=True)\n",
    "```\n",
    "to not limit the number of words in the dictionary, and use character as our unit.\n",
    "\n",
    "**char_level=True** indicates convert input to character.\n",
    "\n",
    "We also need to correct the dictionary because it starts from 1 and that will be a problem later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I1zXYlrxPMPx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique chars: 4762\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "poem_tokenizer = Tokenizer(num_words=None, lower=False, char_level=True) \n",
    "# Create word to ID dictionary\n",
    "poem_tokenizer.fit_on_texts(poems)\n",
    "# Get dictionary\n",
    "word_index = poem_tokenizer.word_index\n",
    "\n",
    "# Note that ID starts from 1!!\n",
    "# We need to add special ID 0\n",
    "word_index[\"<PAD>\"] = 0 # make sure id count equal to word cound\n",
    "# Create ID to word \n",
    "reverse_word_index = dict([(v, k) for (k, v) in word_index.items()])\n",
    "print(\"Number of unique chars: {}\".format(len(word_index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "29iNZFMwKzB-"
   },
   "source": [
    "Again, check if there is any strange symbols in the dictionary. Here we only print first and last parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8Bqy83LtQWjA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PAD> 0\n",
      "不 1\n",
      "人 2\n",
      "一 3\n",
      "山 4\n",
      "风 5\n",
      "无 6\n",
      "花 7\n",
      "来 8\n",
      "日 9\n",
      "春 10\n",
      "爚 4757\n",
      "茏 4758\n",
      "捻 4759\n",
      "窖 4760\n",
      "湍 4761\n"
     ]
    }
   ],
   "source": [
    "# sort word index by ID, and sorting by frequency. So we check the most and least frequency words which most likely\n",
    "# have problem.\n",
    "for (w,i) in sorted(word_index.items(), key=lambda w: w[1]):\n",
    "# print some words to check if there are errors!\n",
    "    if i > 10 and i < len(word_index)-5: continue\n",
    "    print(\"{} {}\".format(w,i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BPVYU_l3qUq_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2818, 13, 81, 78, 6, 714, 10, 1227, 44, 136, 761, 114, 115, 130, 45, 2, 76, 11, 31, 888, 110, 5, 162, 297, 7, 89, 599, 2]\n",
      "汴水东流无限春隋家宫阙已成尘行人莫上长堤望风起杨花愁杀人\n"
     ]
    }
   ],
   "source": [
    "# Apply word to ID on training and test set\n",
    "poems_train = poem_tokenizer.texts_to_sequences(poems_train)\n",
    "poems_test = poem_tokenizer.texts_to_sequences(poems_test)\n",
    "# Check and see if there is any error\n",
    "print(poems_train[0])\n",
    "print(''.join([reverse_word_index[w] for w in poems_train[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nPBqvnAKf0Me"
   },
   "source": [
    "## Prepare the data for input\n",
    "\n",
    "Flatten the input to a long list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "reJzVQlkLgs_"
   },
   "outputs": [],
   "source": [
    "# flatten to a long string of characters\n",
    "poems_train = [w for po in poems_train for w in po]\n",
    "\n",
    "# flatten to a long string of characters\n",
    "poems_test = [w for po in poems_test for w in po]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rj9yNonAf49F"
   },
   "source": [
    "## Define an input object\n",
    "\n",
    "We need to put the input into batches.\n",
    "* Reshape input data into a rectangular matrix and crop remainders\n",
    "* Calculate shape of each batch\n",
    "* Generate batch with input and output = input shift by one time step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YXoDJKqbTYnz"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mNVdwFpgmoeq"
   },
   "outputs": [],
   "source": [
    "class PoemInput(object):\n",
    "    def __init__(self, data, config, name=None):\n",
    "        self.batch_size = batch_size = config.batch_size\n",
    "        self.num_steps = num_steps = config.num_steps\n",
    "        self.epoch_size = ((len(data) // batch_size) - 1) // num_steps # how many batchs\n",
    "        self.sources, self.targets = self.input_producer(\n",
    "            data, batch_size, num_steps, name=name)\n",
    "\n",
    "    def input_producer(self, raw_data, batch_size, num_steps, name=None):\n",
    "        \"\"\"Reshape the poem data to form input and output.\n",
    "        This chunks the raw_data into batches of examples and returns Tensors that\n",
    "        are drawn from these batches.\n",
    "        INPUT:\n",
    "          raw_data: a list of words      # poems_train\n",
    "          batch_size: int, the batch size.\n",
    "          num_steps: int, the sequence length.\n",
    "          name: the name of this operation (optional).\n",
    "        OUTPUT:\n",
    "          A pair of Tensors, each shaped [batch_size, num_steps]. The second element\n",
    "          of the tuple is the same data time-shifted to the right by one.\n",
    "        \"\"\"\n",
    "        raw_data = tf.convert_to_tensor(raw_data, name=\"raw_data\", dtype=tf.int32)\n",
    "        # get size of the 1-d tensor\n",
    "        data_len = tf.size(raw_data)\n",
    "        # calculate how many batches\n",
    "        batch_len = data_len // batch_size\n",
    "        # crop data that does not fit in a batch\n",
    "        data = tf.reshape(raw_data[0:batch_size*batch_len],\n",
    "                          [batch_size, batch_len])\n",
    "        # calculate how many batches in an epoch\n",
    "        epoch_size = (batch_len-1) // num_steps\n",
    "        # make sure there is at least one batch\n",
    "        assertion = tf.assert_positive(epoch_size,\n",
    "            message=\"epoch_size == 0, decrease batch_size or num_steps\")\n",
    "        with tf.control_dependencies([assertion]):\n",
    "            epoch_size = tf.cast(tf.identity(epoch_size, name=\"epoch_size\"), tf.int64)\n",
    "\n",
    "        # start generating slices\n",
    "        # range_input_producer returns a sequence of IDs \n",
    "        i = tf.train.range_input_producer(epoch_size, shuffle=False).dequeue()\n",
    "        x = data[:, i*num_steps  :(i+1)*num_steps]\n",
    "        y = data[:, i*num_steps+1:(i+1)*num_steps+1]\n",
    "        print(\"input_ source type: \", type(x))   # tensor\n",
    "        print(\"input_ source shape: \", x.shape) # batch_size, num_steps, epoch_size\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6k4i6WJ8gFLl"
   },
   "source": [
    "## Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QhBnCcTTUpLS"
   },
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "class Hparam(object):\n",
    "    learning_rate = 1.0\n",
    "    max_grad_norm = 5\n",
    "    num_layers = 1\n",
    "    num_steps = 35 # how many words in each training; affect training time highly\n",
    "    vocab_size = len(word_index)\n",
    "    embedding_size = 100\n",
    "    hidden_size = 100 # LSTM hidden lalyer size\n",
    "    warmup_epochs = 3 # first 3 epoch, learning rate fixed, after this, learning rate decresing\n",
    "    num_epochs_to_train = 5\n",
    "    keep_prob = 0.6 # random drop out 40% data\n",
    "    lr_decay = 0.9 # learning rate decreses 0.1 every time\n",
    "    batch_size = 100 # depend on hardware, cpu or gpu\n",
    "\n",
    "config = Hparam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WyOWLTIVgJWZ"
   },
   "source": [
    "## Construct model\n",
    "In this step, the entire model structure must be defined completely. Including\n",
    "* Input\n",
    "* Size of layers\n",
    "* Connection between layers\n",
    "* Variables in layers\n",
    "* Output\n",
    "* Loss\n",
    "* Operations that apply the gradients (optimizer)\n",
    "* Placeholder for feeding special values\n",
    "* Properties that can be read from outside\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.cudnn_rnn import CudnnLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5tuOJamJS6gm"
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.rnn import BasicLSTMCell, MultiRNNCell\n",
    "from tensorflow.nn import embedding_lookup, dropout\n",
    "\n",
    "# Build our model\n",
    "class MySeq2SeqModel(object):\n",
    "    def __init__(self, is_training, config, input_):\n",
    "        self._is_training = is_training\n",
    "        self._input = input_\n",
    "        self._cell = None\n",
    "        self.batch_size = input_.batch_size\n",
    "        self.num_steps = input_.num_steps\n",
    "        rnn_size = config.hidden_size\n",
    "        vocab_size = config.vocab_size\n",
    "        embedding_size = config.embedding_size\n",
    "\n",
    "        # Embeddings can only exist on CPU\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            embedding_weights = tf.get_variable(\"embedding\", \\\n",
    "                         [vocab_size, embedding_size]) # 4762 * 100\n",
    "            # take input_.sources from embedding_weights, convert wordID to embedding\n",
    "            embed_inputs = tf.nn.embedding_lookup(embedding_weights, input_.sources) \n",
    "            print('embed_inputs shape: ', embed_inputs.shape) # batch_size, num_steps, epoch_size, embedding_size\n",
    "\n",
    "        if is_training and config.keep_prob < 1.:\n",
    "            embed_inputs = tf.nn.dropout(embed_inputs, config.keep_prob)\n",
    "\n",
    "        # build RNN using CudnnLSTM\n",
    "        output, _ = self._build_rnn(embed_inputs, config, is_training)\n",
    "        # build RNN using basic LSTM\n",
    "#         output, _ = self._build_rnn_old_lstm(embed_inputs, config, is_training) # _ for state\n",
    "\n",
    "        # Remember RNN output is [batch_size x time, rnnsize]\n",
    "        # Dense layer for projecting onto vocabulary size\n",
    "        softmax_w = tf.get_variable(\"softmax_w\", [rnn_size, vocab_size])\n",
    "        softmax_b = tf.get_variable(\"softmax_b\", [vocab_size])\n",
    "        logits = tf.nn.xw_plus_b(output, softmax_w, softmax_b)\n",
    "        # Reshape logits to be a 3-D tensor for sequence loss\n",
    "        logits = tf.reshape(logits, [self.batch_size, self.num_steps, vocab_size])\n",
    "        self._logits = logits\n",
    "\n",
    "        # Use the contrib sequence loss and average over the batches\n",
    "        loss = tf.contrib.seq2seq.sequence_loss(\n",
    "            logits,\n",
    "            input_.targets,\n",
    "            tf.ones([self.batch_size, self.num_steps]), # weight\n",
    "            average_across_timesteps=False, \n",
    "            average_across_batch=True)\n",
    "        \n",
    "        # loss is a sequence by time step\n",
    "        \n",
    "        # Update the cost\n",
    "        self._cost = tf.reduce_sum(loss)\n",
    "\n",
    "        if not is_training:\n",
    "            return\n",
    "\n",
    "        # A variable to store learning rate\n",
    "        self._lr = tf.Variable(0.0, trainable=False) # avoid make gradient of learning rate\n",
    "\n",
    "        # Calculate gradients\n",
    "        # Get a list of trainable variables\n",
    "        tvars = tf.trainable_variables()\n",
    "        # Get gradient and clip by norm\n",
    "        grads, _ = tf.clip_by_global_norm(\\\n",
    "                     tf.gradients(self._cost, tvars),\n",
    "                     config.max_grad_norm) # control all gradients in max_grad_norm\n",
    "        # Define an optimizer\n",
    "        # Note that the optimizer reads the value of learning rate from variable\n",
    "        optimizer = tf.train.GradientDescentOptimizer(self._lr) # control optimizer's learning rate\n",
    "        # Define an operation that actually applies the gradients\n",
    "        self._train_op = optimizer.apply_gradients(\n",
    "            zip(grads, tvars),\n",
    "            global_step=tf.train.get_or_create_global_step())\n",
    "        # A placeholder for feeding new learning rates\n",
    "        self._new_lr = tf.placeholder(\n",
    "             tf.float32, shape=[], name=\"new_learning_rate\")\n",
    "        self._lr_update_op = tf.assign(self._lr, self._new_lr)\n",
    "  \n",
    "    def _build_rnn(self, inputs, config, is_training):\n",
    "        # RNN requires time-major\n",
    "        inputs = tf.transpose(inputs, [1, 0, 2]) # replace batch_size with time\n",
    "        self._cell = CudnnLSTM(\n",
    "            num_layers=config.num_layers,\n",
    "            num_units=config.hidden_size,\n",
    "            )\n",
    "        self._cell.build(inputs.get_shape()) \n",
    "        outputs, state = self._cell(inputs)\n",
    "        # Transpose from time-major to batch-major\n",
    "        outputs = tf.transpose(outputs, [1, 0, 2]) # replace back\n",
    "        # Reshape from [batch, time, rnnsize] to [batch x time, rnnsize]\n",
    "        # For computing softmax later\n",
    "        outputs = tf.reshape(outputs, [-1, config.hidden_size])\n",
    "        return outputs, state\n",
    "\n",
    "    def _build_rnn_old_lstm(self, inputs, config, is_training):\n",
    "        def make_cell():\n",
    "            cell = BasicLSTMCell(\n",
    "            config.hidden_size, forget_bias=0.0, state_is_tuple=True,\n",
    "            reuse=not is_training)\n",
    "            if is_training and config.keep_prob < 1:\n",
    "                cell = tf.contrib.rnn.DropoutWrapper(\n",
    "                    cell, output_keep_prob=config.keep_prob)\n",
    "            return cell\n",
    "\n",
    "        cell = tf.contrib.rnn.MultiRNNCell(\n",
    "            [make_cell() for _ in range(config.num_layers)], state_is_tuple=True)\n",
    "\n",
    "        self._initial_state = cell.zero_state(config.batch_size, tf.float32)\n",
    "        state = self._initial_state\n",
    "        outputs = []\n",
    "        inputs = tf.unstack(inputs, num=self.num_steps, axis=1)\n",
    "        outputs, state = tf.nn.static_rnn(cell, inputs,\n",
    "                                          initial_state=self._initial_state)\n",
    "        output = tf.reshape(tf.concat(outputs, 1), [-1, config.hidden_size])\n",
    "        return output, state\n",
    "\n",
    "    def assign_lr(self, session, lr_value):\n",
    "        session.run(self._lr_update_op, feed_dict={self._new_lr: lr_value})\n",
    "\n",
    "    @property\n",
    "    def input(self):\n",
    "        return self._input\n",
    "\n",
    "    @property\n",
    "    def cost(self):\n",
    "        return self._cost\n",
    "\n",
    "    @property\n",
    "    def lr(self):\n",
    "        return self._lr\n",
    "\n",
    "    @property\n",
    "    def train_op(self):\n",
    "        return self._train_op\n",
    "\n",
    "    @property\n",
    "    def logits(self):\n",
    "        return self._logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7EYipAzwgXMD"
   },
   "source": [
    "## Define a training operation for an epoch\n",
    "This procedure gets the output from the model for each batch.\n",
    "We need a dictionary with these keys:\n",
    "\n",
    "* \"cost\": Reads the propertie `model.cost` that we defined above. \n",
    "* \"do_op\": Perform operation `model.train_op` that applies gradients\n",
    "\n",
    "After running (calling `session.run()`), the same key will contain the return values.\n",
    "\n",
    "We can add any key in the dictionary that corresponds to `@property` in the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "84z2L2sggp48"
   },
   "outputs": [],
   "source": [
    "def run_epoch(session, model, do_op=None, verbose=False):\n",
    "    start_time = time.time()\n",
    "    costs = 0.0\n",
    "    iters = 0\n",
    "    feed_to_model_dict = {\n",
    "        \"cost\": model.cost,\n",
    "    }\n",
    "    # if an operation is provided, put that in the feed\n",
    "    if do_op is not None:\n",
    "        feed_to_model_dict[\"do_op\"] = do_op\n",
    "\n",
    "    for step in range(model.input.epoch_size):\n",
    "        # use the session to run, feed the dictionary\n",
    "        s_out = session.run(feed_to_model_dict)\n",
    "        # The returned dictionary will contain the information we need\n",
    "        cost = s_out[\"cost\"]\n",
    "        # Accumulate cost\n",
    "        costs += cost\n",
    "        # Accumulate number of training steps\n",
    "        iters += model.input.num_steps\n",
    "        # Print loss periodically\n",
    "        if verbose and (step+1) % (model.input.epoch_size // 5) == 0:\n",
    "            print(\"%.0f%% ppl: %.3f, speed: %.0f char/sec\" %\n",
    "                ((step+1) * 100.0 / model.input.epoch_size, \\\n",
    "                 np.exp(costs/iters), \\\n",
    "                 iters * model.input.batch_size/(time.time() - start_time)))\n",
    "\n",
    "    return np.exp(costs / iters) # costs are natural log value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NHRlVdNelCkx"
   },
   "source": [
    "## Define a Generator\n",
    "We will also create a Generator Model to generate new poems. Note that it is much less complicated than the training model.\n",
    "However, we need to add a procedure to generate output for some steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sZ0VsXVuqBM1"
   },
   "outputs": [],
   "source": [
    "class MyGeneratorModel(object):\n",
    "    def __init__(self, config):\n",
    "        self._input = tf.placeholder(tf.int32, shape=[1], name=\"_input\")\n",
    "        self.batch_size = 1\n",
    "        self.num_steps = config.num_steps\n",
    "        rnn_size = config.hidden_size\n",
    "        vocab_size = config.vocab_size\n",
    "        embedding_size = config.embedding_size\n",
    "\n",
    "        # Embeddings can only exist on CPU\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            embedding_weights = tf.get_variable(\"embedding\", \\\n",
    "                         [vocab_size, embedding_size])\n",
    "            embed_inputs = tf.nn.embedding_lookup(embedding_weights, self._input)\n",
    "            embed_inputs = tf.expand_dims(embed_inputs, 0)\n",
    "\n",
    "        # build RNN using CudnnLSTM\n",
    "        self._cell = CudnnLSTM(\n",
    "            num_layers=config.num_layers,\n",
    "            num_units=config.hidden_size,\n",
    "            )\n",
    "\n",
    "        # build final projection layer\n",
    "        softmax_w = tf.get_variable(\"softmax_w\", [rnn_size, vocab_size])\n",
    "        softmax_b = tf.get_variable(\"softmax_b\", [vocab_size])\n",
    "\n",
    "        # Collect a sequence of output word IDs\n",
    "        self._output_word_ids = []\n",
    "\n",
    "        # Decode first word\n",
    "        outputs, state = self._cell(embed_inputs)\n",
    "        outputs = tf.reshape(outputs, [-1, config.hidden_size])\n",
    "        logits = tf.nn.xw_plus_b(outputs, softmax_w, softmax_b)\n",
    "        # Get input for next step\n",
    "        next_input = tf.argmax(logits, axis=-1)\n",
    "        next_input = tf.squeeze(next_input)\n",
    "        self._output_word_ids.append(next_input)\n",
    "        # Convert next input to word embeddings\n",
    "        next_input = tf.nn.embedding_lookup(embedding_weights, next_input)\n",
    "        next_input = tf.reshape(next_input, [1, 1, embedding_size])\n",
    "\n",
    "        # Feed back to LSTM\n",
    "        for _ in range(self.num_steps-1):\n",
    "            outputs, state = self._cell(next_input, state)\n",
    "            outputs = tf.reshape(outputs, [-1, config.hidden_size])\n",
    "            logits = tf.nn.xw_plus_b(outputs, softmax_w, softmax_b)\n",
    "            next_input = tf.argmax(logits, axis=-1)\n",
    "            next_input = tf.squeeze(next_input)\n",
    "            self._output_word_ids.append(next_input)\n",
    "\n",
    "            next_input = tf.nn.embedding_lookup(embedding_weights, next_input)\n",
    "            next_input = tf.reshape(next_input, [1, 1, embedding_size])\n",
    "\n",
    "    @property\n",
    "    def output_word_ids(self):\n",
    "        return self._output_word_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-oOtiwCr0lrD"
   },
   "source": [
    "## Define a call to generator\n",
    "Again we need a decoder to translate word IDs back to words. And we need to define a procedure to communicate with the model. `feed_dict` and `fetches` are two keys to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CB9TLfWilDg9"
   },
   "outputs": [],
   "source": [
    "def decode_text(text, max_len_newline=5):\n",
    "    words = [reverse_word_index.get(i, \"<UNK>\") for i in text]\n",
    "    fixed_width_string = []\n",
    "\n",
    "    for w_pos in range(len(words)):\n",
    "        fixed_width_string.append(words[w_pos])\n",
    "        if (w_pos+1) % max_len_newline == 0:\n",
    "            fixed_width_string.append('\\n')\n",
    "    return ''.join(fixed_width_string)\n",
    "\n",
    "def run_generator(session, model, seed_word, config):\n",
    "  \n",
    "    feed_to_model_dict = {\n",
    "        model._input: [seed_word],\n",
    "    }\n",
    "    fetch_model_dict = {\n",
    "        \"output_word_ids\": model.output_word_ids\n",
    "    }\n",
    "\n",
    "    # An example of sending and receiving data from the model\n",
    "    vals = session.run(fetches=fetch_model_dict, feed_dict=feed_to_model_dict)\n",
    "    output_word_ids = vals['output_word_ids']\n",
    "\n",
    "    # Decode to readable words\n",
    "    print(decode_text([seed_word] + output_word_ids, (config.num_steps+1)//4))\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qvm0wW2inUwk"
   },
   "source": [
    "## Main training controller\n",
    "Finally, we define a controller that:\n",
    "* Create the model for training\n",
    "* Create the model for testing, copying from the training model\n",
    "* Prepare the input data\n",
    "* Define what to log in the progress of training\n",
    "* Create a `session` that communicates with computation graph\n",
    "* Change learning rate optionally\n",
    "* Get test set results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hKsN1qiVhdms"
   },
   "outputs": [],
   "source": [
    "def main(_):\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        initializer = tf.random_uniform_initializer(-0.1, 0.1)\n",
    "\n",
    "        with tf.name_scope(\"Train\"):\n",
    "            # Create input producer\n",
    "            train_input = PoemInput(poems_train, config, name=\"TrainInput\")\n",
    "            # Create the model instance\n",
    "            with tf.variable_scope(\"Model\", reuse=None, initializer=initializer):\n",
    "                m = MySeq2SeqModel(is_training=True, config=config, input_=train_input)\n",
    "            # Add information to logs\n",
    "            tf.summary.scalar(\"Training_Loss\", m.cost)\n",
    "            tf.summary.scalar(\"Learning_Rate\", m.lr)\n",
    "\n",
    "        with tf.name_scope(\"Test\"):\n",
    "            eval_config = Hparam()\n",
    "            eval_config.batch_size = 1\n",
    "            eval_config.num_steps = 20\n",
    "\n",
    "            # Create another input for test data\n",
    "            # Note that eval_config was set locally\n",
    "            test_input = PoemInput(poems_test, eval_config, name=\"TestInput\")\n",
    "            # Create another model but reuse the variables in the training model\n",
    "            with tf.variable_scope(\"Model\", reuse=True):\n",
    "                mtest = MySeq2SeqModel(is_training=False, config=eval_config,\n",
    "                             input_=test_input)\n",
    "\n",
    "        with tf.name_scope(\"Gen\"):\n",
    "            generator_config = Hparam()\n",
    "            generator_config.batch_size = 1\n",
    "            generator_config.num_steps = 19\n",
    "            # Create generator model\n",
    "            with tf.variable_scope(\"Model\", reuse=True):\n",
    "                mgenerate = MyGeneratorModel(config=generator_config)\n",
    "\n",
    "        # Hardware settings\n",
    "        config_proto = tf.ConfigProto(allow_soft_placement=True)\n",
    "        # Create a MonitoredTrainingSession that controls the training process\n",
    "        # Also automatically logs and reports \n",
    "        # Note the `checkpoint_dir` setting\n",
    "        with tf.train.MonitoredTrainingSession(checkpoint_dir=\"logs\", \\\n",
    "                                               config=config_proto, \\\n",
    "                                               log_step_count_steps=-1) as session:\n",
    "            for i in range(config.num_epochs_to_train):\n",
    "\n",
    "                # Calculate learning rate decay\n",
    "                lr_decay = config.lr_decay ** max(i + 1 - config.warmup_epochs, 0.0)\n",
    "                # Set learning rate\n",
    "                m.assign_lr(session, config.learning_rate * lr_decay)\n",
    "                # Print new learning rate\n",
    "                print(\"Epoch: %d Learning rate: %.3f\" % (i + 1, session.run(m.lr)))\n",
    "                # Train one epoch and report loss\n",
    "                train_perplexity = run_epoch(session, m, do_op=m.train_op, verbose=True)\n",
    "                print(\"Epoch: %d Train Perplexity: %.3f\" % (i + 1, train_perplexity))\n",
    "\n",
    "          # End of training\n",
    "          # Evaluate test set performance\n",
    "            test_perplexity = run_epoch(session, mtest)\n",
    "            print(\"Test Perplexity: %.3f\" % test_perplexity)\n",
    "\n",
    "          # Set a seed word and generate new poem\n",
    "            seed_word = '天'\n",
    "            run_generator(session, mgenerate, seed_word=word_index[seed_word], config=generator_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g_6KfSfUnbC5"
   },
   "source": [
    "## Start training\n",
    "We can actually start training by calling the controller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y3f2BoLtl9Yo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-36557305d1ad>:40: range_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.range(limit).shuffle(limit).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /opt/anaconda/lib/python3.6/site-packages/tensorflow/python/training/input.py:318: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /opt/anaconda/lib/python3.6/site-packages/tensorflow/python/training/input.py:188: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
      "WARNING:tensorflow:From /opt/anaconda/lib/python3.6/site-packages/tensorflow/python/training/input.py:197: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /opt/anaconda/lib/python3.6/site-packages/tensorflow/python/training/input.py:197: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "input_ source type:  <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "input_ source shape:  (100, ?)\n",
      "embed_inputs shape:  (100, ?, 100)\n",
      "input_ source type:  <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "input_ source shape:  (1, ?)\n",
      "embed_inputs shape:  (1, ?, 100)\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "No OpKernel was registered to support Op 'CudnnRNNCanonicalToParams' with these attrs.  Registered devices: [CPU,XLA_CPU,XLA_GPU], Registered kernels:\n  device='GPU'; T in [DT_DOUBLE]\n  device='GPU'; T in [DT_FLOAT]\n  device='GPU'; T in [DT_HALF]\n\n\t [[node Train/Model/cudnn_lstm_1/CudnnRNNCanonicalToParams (defined at /opt/anaconda/lib/python3.6/site-packages/tensorflow/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py:1251)  = CudnnRNNCanonicalToParams[T=DT_FLOAT, direction=\"unidirectional\", dropout=0, input_mode=\"linear_input\", num_params=8, rnn_mode=\"lstm\", seed=0, seed2=0, _device=\"/device:GPU:0\"](Train/Model/cudnn_lstm_1/CudnnRNNCanonicalToParams/num_layers, Train/Model/cudnn_lstm_1/CudnnRNNCanonicalToParams/num_units, Train/Model/cudnn_lstm_1/CudnnRNNCanonicalToParams/input_size, Train/Model/cudnn_lstm_1/random_uniform, Train/Model/cudnn_lstm_1/random_uniform_1, Train/Model/cudnn_lstm_1/random_uniform_2, Train/Model/cudnn_lstm_1/random_uniform_3, Train/Model/cudnn_lstm_1/random_uniform_4, Train/Model/cudnn_lstm_1/random_uniform_5, Train/Model/cudnn_lstm_1/random_uniform_6, Train/Model/cudnn_lstm_1/random_uniform_7, Train/Model/cudnn_lstm_1/Const, Train/Model/cudnn_lstm_1/Const_1, Train/Model/cudnn_lstm_1/Const_2, Train/Model/cudnn_lstm_1/Const_3, Train/Model/cudnn_lstm_1/Const_4, Train/Model/cudnn_lstm_1/Const_5, Train/Model/cudnn_lstm_1/Const_6, Train/Model/cudnn_lstm_1/Const_7)]]\n\nCaused by op 'Train/Model/cudnn_lstm_1/CudnnRNNCanonicalToParams', defined at:\n  File \"/opt/anaconda/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/opt/anaconda/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/opt/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/opt/anaconda/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/opt/anaconda/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/opt/anaconda/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/opt/anaconda/lib/python3.6/asyncio/base_events.py\", line 438, in run_forever\n    self._run_once()\n  File \"/opt/anaconda/lib/python3.6/asyncio/base_events.py\", line 1451, in _run_once\n    handle._run()\n  File \"/opt/anaconda/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/opt/anaconda/lib/python3.6/site-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/opt/anaconda/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/opt/anaconda/lib/python3.6/site-packages/tornado/gen.py\", line 1233, in inner\n    self.run()\n  File \"/opt/anaconda/lib/python3.6/site-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/opt/anaconda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 370, in dispatch_queue\n    yield self.process_one()\n  File \"/opt/anaconda/lib/python3.6/site-packages/tornado/gen.py\", line 346, in wrapper\n    runner = Runner(result, future, yielded)\n  File \"/opt/anaconda/lib/python3.6/site-packages/tornado/gen.py\", line 1080, in __init__\n    self.run()\n  File \"/opt/anaconda/lib/python3.6/site-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/opt/anaconda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/opt/anaconda/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/opt/anaconda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/opt/anaconda/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/opt/anaconda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/opt/anaconda/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/opt/anaconda/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/opt/anaconda/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/opt/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2843, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/opt/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2869, in _run_cell\n    return runner(coro)\n  File \"/opt/anaconda/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/opt/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3044, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/opt/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3215, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/opt/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3291, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-17-0a902aad3aed>\", line 1, in <module>\n    main(1)\n  File \"<ipython-input-16-0d7053a87609>\", line 11, in main\n    m = MySeq2SeqModel(is_training=True, config=config, input_=train_input)\n  File \"<ipython-input-12-70e2c2bb3d52>\", line 28, in __init__\n    output, _ = self._build_rnn(embed_inputs, config, is_training)\n  File \"<ipython-input-12-70e2c2bb3d52>\", line 86, in _build_rnn\n    self._cell.build(inputs.get_shape())\n  File \"/opt/anaconda/lib/python3.6/site-packages/tensorflow/contrib/cudnn_rnn/python/layers/cudnn_rnn.py\", line 352, in build\n    opaque_params_t = self._canonical_to_opaque(weights, biases)\n  File \"/opt/anaconda/lib/python3.6/site-packages/tensorflow/contrib/cudnn_rnn/python/layers/cudnn_rnn.py\", line 474, in _canonical_to_opaque\n    direction=self._direction)\n  File \"/opt/anaconda/lib/python3.6/site-packages/tensorflow/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py\", line 1251, in cudnn_rnn_canonical_to_opaque_params\n    name=name)\n  File \"/opt/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/gen_cudnn_rnn_ops.py\", line 642, in cudnn_rnn_canonical_to_params\n    name=name)\n  File \"/opt/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/opt/anaconda/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"/opt/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"/opt/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): No OpKernel was registered to support Op 'CudnnRNNCanonicalToParams' with these attrs.  Registered devices: [CPU,XLA_CPU,XLA_GPU], Registered kernels:\n  device='GPU'; T in [DT_DOUBLE]\n  device='GPU'; T in [DT_FLOAT]\n  device='GPU'; T in [DT_HALF]\n\n\t [[node Train/Model/cudnn_lstm_1/CudnnRNNCanonicalToParams (defined at /opt/anaconda/lib/python3.6/site-packages/tensorflow/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py:1251)  = CudnnRNNCanonicalToParams[T=DT_FLOAT, direction=\"unidirectional\", dropout=0, input_mode=\"linear_input\", num_params=8, rnn_mode=\"lstm\", seed=0, seed2=0, _device=\"/device:GPU:0\"](Train/Model/cudnn_lstm_1/CudnnRNNCanonicalToParams/num_layers, Train/Model/cudnn_lstm_1/CudnnRNNCanonicalToParams/num_units, Train/Model/cudnn_lstm_1/CudnnRNNCanonicalToParams/input_size, Train/Model/cudnn_lstm_1/random_uniform, Train/Model/cudnn_lstm_1/random_uniform_1, Train/Model/cudnn_lstm_1/random_uniform_2, Train/Model/cudnn_lstm_1/random_uniform_3, Train/Model/cudnn_lstm_1/random_uniform_4, Train/Model/cudnn_lstm_1/random_uniform_5, Train/Model/cudnn_lstm_1/random_uniform_6, Train/Model/cudnn_lstm_1/random_uniform_7, Train/Model/cudnn_lstm_1/Const, Train/Model/cudnn_lstm_1/Const_1, Train/Model/cudnn_lstm_1/Const_2, Train/Model/cudnn_lstm_1/Const_3, Train/Model/cudnn_lstm_1/Const_4, Train/Model/cudnn_lstm_1/Const_5, Train/Model/cudnn_lstm_1/Const_6, Train/Model/cudnn_lstm_1/Const_7)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1316\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1317\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1351\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1352\u001b[0;31m       \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: No OpKernel was registered to support Op 'CudnnRNNCanonicalToParams' with these attrs.  Registered devices: [CPU,XLA_CPU,XLA_GPU], Registered kernels:\n  device='GPU'; T in [DT_DOUBLE]\n  device='GPU'; T in [DT_FLOAT]\n  device='GPU'; T in [DT_HALF]\n\n\t [[{{node Train/Model/cudnn_lstm_1/CudnnRNNCanonicalToParams}} = CudnnRNNCanonicalToParams[T=DT_FLOAT, direction=\"unidirectional\", dropout=0, input_mode=\"linear_input\", num_params=8, rnn_mode=\"lstm\", seed=0, seed2=0, _device=\"/device:GPU:0\"](Train/Model/cudnn_lstm_1/CudnnRNNCanonicalToParams/num_layers, Train/Model/cudnn_lstm_1/CudnnRNNCanonicalToParams/num_units, Train/Model/cudnn_lstm_1/CudnnRNNCanonicalToParams/input_size, Train/Model/cudnn_lstm_1/random_uniform, Train/Model/cudnn_lstm_1/random_uniform_1, Train/Model/cudnn_lstm_1/random_uniform_2, Train/Model/cudnn_lstm_1/random_uniform_3, Train/Model/cudnn_lstm_1/random_uniform_4, Train/Model/cudnn_lstm_1/random_uniform_5, Train/Model/cudnn_lstm_1/random_uniform_6, Train/Model/cudnn_lstm_1/random_uniform_7, Train/Model/cudnn_lstm_1/Const, Train/Model/cudnn_lstm_1/Const_1, Train/Model/cudnn_lstm_1/Const_2, Train/Model/cudnn_lstm_1/Const_3, Train/Model/cudnn_lstm_1/Const_4, Train/Model/cudnn_lstm_1/Const_5, Train/Model/cudnn_lstm_1/Const_6, Train/Model/cudnn_lstm_1/Const_7)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-0a902aad3aed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-0d7053a87609>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(_)\u001b[0m\n\u001b[1;32m     42\u001b[0m         with tf.train.MonitoredTrainingSession(checkpoint_dir=\"logs\", \\\n\u001b[1;32m     43\u001b[0m                                                \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig_proto\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                                                log_step_count_steps=-1) as session:\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_epochs_to_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mMonitoredTrainingSession\u001b[0;34m(master, is_chief, checkpoint_dir, scaffold, hooks, chief_only_hooks, save_checkpoint_secs, save_summaries_steps, save_summaries_secs, config, stop_grace_period_secs, log_step_count_steps, max_wait_secs, save_checkpoint_steps, summary_dir)\u001b[0m\n\u001b[1;32m    502\u001b[0m       \u001b[0msession_creator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession_creator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m       \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_hooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m       stop_grace_period_secs=stop_grace_period_secs)\n\u001b[0m\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, session_creator, hooks, stop_grace_period_secs)\u001b[0m\n\u001b[1;32m    919\u001b[0m     super(MonitoredSession, self).__init__(\n\u001b[1;32m    920\u001b[0m         \u001b[0msession_creator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshould_recover\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         stop_grace_period_secs=stop_grace_period_secs)\n\u001b[0m\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, session_creator, hooks, should_recover, stop_grace_period_secs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         stop_grace_period_secs=stop_grace_period_secs)\n\u001b[1;32m    642\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshould_recover\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_RecoverableSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coordinated_creator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coordinated_creator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sess_creator)\u001b[0m\n\u001b[1;32m   1105\u001b[0m     \"\"\"\n\u001b[1;32m   1106\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess_creator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess_creator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1107\u001b[0;31m     \u001b[0m_WrappedSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_create_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36m_create_session\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1110\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess_creator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1113\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m         logging.info('An error was raised while a session was being created. '\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mcreate_session\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    798\u001b[0m       \u001b[0;34m\"\"\"Creates a coordinated session.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m       \u001b[0;31m# Keep the tf_sess for unit testing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_sess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_creator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    801\u001b[0m       \u001b[0;31m# We don't want coordinator to suppress any exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoord\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoordinator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCoordinator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_stop_exception_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mcreate_session\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0minit_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scaffold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0minit_feed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scaffold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_feed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m         init_fn=self._scaffold.init_fn)\n\u001b[0m\u001b[1;32m    567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/tensorflow/python/training/session_manager.py\u001b[0m in \u001b[0;36mprepare_session\u001b[0;34m(self, master, init_op, saver, checkpoint_dir, checkpoint_filename_with_path, wait_for_checkpoint, max_wait_secs, config, init_feed_dict, init_fn)\u001b[0m\n\u001b[1;32m    292\u001b[0m                            \"init_fn or local_init_op was given\")\n\u001b[1;32m    293\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0minit_op\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit_feed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0minit_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0minit_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: No OpKernel was registered to support Op 'CudnnRNNCanonicalToParams' with these attrs.  Registered devices: [CPU,XLA_CPU,XLA_GPU], Registered kernels:\n  device='GPU'; T in [DT_DOUBLE]\n  device='GPU'; T in [DT_FLOAT]\n  device='GPU'; T in [DT_HALF]\n\n\t [[node Train/Model/cudnn_lstm_1/CudnnRNNCanonicalToParams (defined at /opt/anaconda/lib/python3.6/site-packages/tensorflow/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py:1251)  = CudnnRNNCanonicalToParams[T=DT_FLOAT, direction=\"unidirectional\", dropout=0, input_mode=\"linear_input\", num_params=8, rnn_mode=\"lstm\", seed=0, seed2=0, _device=\"/device:GPU:0\"](Train/Model/cudnn_lstm_1/CudnnRNNCanonicalToParams/num_layers, Train/Model/cudnn_lstm_1/CudnnRNNCanonicalToParams/num_units, Train/Model/cudnn_lstm_1/CudnnRNNCanonicalToParams/input_size, Train/Model/cudnn_lstm_1/random_uniform, Train/Model/cudnn_lstm_1/random_uniform_1, Train/Model/cudnn_lstm_1/random_uniform_2, Train/Model/cudnn_lstm_1/random_uniform_3, Train/Model/cudnn_lstm_1/random_uniform_4, Train/Model/cudnn_lstm_1/random_uniform_5, Train/Model/cudnn_lstm_1/random_uniform_6, Train/Model/cudnn_lstm_1/random_uniform_7, Train/Model/cudnn_lstm_1/Const, Train/Model/cudnn_lstm_1/Const_1, Train/Model/cudnn_lstm_1/Const_2, Train/Model/cudnn_lstm_1/Const_3, Train/Model/cudnn_lstm_1/Const_4, Train/Model/cudnn_lstm_1/Const_5, Train/Model/cudnn_lstm_1/Const_6, Train/Model/cudnn_lstm_1/Const_7)]]\n\nCaused by op 'Train/Model/cudnn_lstm_1/CudnnRNNCanonicalToParams', defined at:\n  File \"/opt/anaconda/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/opt/anaconda/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/opt/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/opt/anaconda/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/opt/anaconda/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/opt/anaconda/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/opt/anaconda/lib/python3.6/asyncio/base_events.py\", line 438, in run_forever\n    self._run_once()\n  File \"/opt/anaconda/lib/python3.6/asyncio/base_events.py\", line 1451, in _run_once\n    handle._run()\n  File \"/opt/anaconda/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/opt/anaconda/lib/python3.6/site-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/opt/anaconda/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/opt/anaconda/lib/python3.6/site-packages/tornado/gen.py\", line 1233, in inner\n    self.run()\n  File \"/opt/anaconda/lib/python3.6/site-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/opt/anaconda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 370, in dispatch_queue\n    yield self.process_one()\n  File \"/opt/anaconda/lib/python3.6/site-packages/tornado/gen.py\", line 346, in wrapper\n    runner = Runner(result, future, yielded)\n  File \"/opt/anaconda/lib/python3.6/site-packages/tornado/gen.py\", line 1080, in __init__\n    self.run()\n  File \"/opt/anaconda/lib/python3.6/site-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/opt/anaconda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/opt/anaconda/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/opt/anaconda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/opt/anaconda/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/opt/anaconda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/opt/anaconda/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/opt/anaconda/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/opt/anaconda/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/opt/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2843, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/opt/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2869, in _run_cell\n    return runner(coro)\n  File \"/opt/anaconda/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/opt/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3044, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/opt/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3215, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/opt/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3291, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-17-0a902aad3aed>\", line 1, in <module>\n    main(1)\n  File \"<ipython-input-16-0d7053a87609>\", line 11, in main\n    m = MySeq2SeqModel(is_training=True, config=config, input_=train_input)\n  File \"<ipython-input-12-70e2c2bb3d52>\", line 28, in __init__\n    output, _ = self._build_rnn(embed_inputs, config, is_training)\n  File \"<ipython-input-12-70e2c2bb3d52>\", line 86, in _build_rnn\n    self._cell.build(inputs.get_shape())\n  File \"/opt/anaconda/lib/python3.6/site-packages/tensorflow/contrib/cudnn_rnn/python/layers/cudnn_rnn.py\", line 352, in build\n    opaque_params_t = self._canonical_to_opaque(weights, biases)\n  File \"/opt/anaconda/lib/python3.6/site-packages/tensorflow/contrib/cudnn_rnn/python/layers/cudnn_rnn.py\", line 474, in _canonical_to_opaque\n    direction=self._direction)\n  File \"/opt/anaconda/lib/python3.6/site-packages/tensorflow/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py\", line 1251, in cudnn_rnn_canonical_to_opaque_params\n    name=name)\n  File \"/opt/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/gen_cudnn_rnn_ops.py\", line 642, in cudnn_rnn_canonical_to_params\n    name=name)\n  File \"/opt/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/opt/anaconda/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"/opt/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"/opt/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): No OpKernel was registered to support Op 'CudnnRNNCanonicalToParams' with these attrs.  Registered devices: [CPU,XLA_CPU,XLA_GPU], Registered kernels:\n  device='GPU'; T in [DT_DOUBLE]\n  device='GPU'; T in [DT_FLOAT]\n  device='GPU'; T in [DT_HALF]\n\n\t [[node Train/Model/cudnn_lstm_1/CudnnRNNCanonicalToParams (defined at /opt/anaconda/lib/python3.6/site-packages/tensorflow/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py:1251)  = CudnnRNNCanonicalToParams[T=DT_FLOAT, direction=\"unidirectional\", dropout=0, input_mode=\"linear_input\", num_params=8, rnn_mode=\"lstm\", seed=0, seed2=0, _device=\"/device:GPU:0\"](Train/Model/cudnn_lstm_1/CudnnRNNCanonicalToParams/num_layers, Train/Model/cudnn_lstm_1/CudnnRNNCanonicalToParams/num_units, Train/Model/cudnn_lstm_1/CudnnRNNCanonicalToParams/input_size, Train/Model/cudnn_lstm_1/random_uniform, Train/Model/cudnn_lstm_1/random_uniform_1, Train/Model/cudnn_lstm_1/random_uniform_2, Train/Model/cudnn_lstm_1/random_uniform_3, Train/Model/cudnn_lstm_1/random_uniform_4, Train/Model/cudnn_lstm_1/random_uniform_5, Train/Model/cudnn_lstm_1/random_uniform_6, Train/Model/cudnn_lstm_1/random_uniform_7, Train/Model/cudnn_lstm_1/Const, Train/Model/cudnn_lstm_1/Const_1, Train/Model/cudnn_lstm_1/Const_2, Train/Model/cudnn_lstm_1/Const_3, Train/Model/cudnn_lstm_1/Const_4, Train/Model/cudnn_lstm_1/Const_5, Train/Model/cudnn_lstm_1/Const_6, Train/Model/cudnn_lstm_1/Const_7)]]\n"
     ]
    }
   ],
   "source": [
    "main(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YgU5M-MXVB9x"
   },
   "source": [
    "We can see the training process shown here. Observe that training loss keeps decreasing, which means that the model is actually learning. \n",
    "\n",
    "Also, due to the speedup of CudnnLSTM, the speed can be very fast (> 100,000 w/s). Using basic LSTM can only achieve ~6,000 w/s.\n",
    "\n",
    "If you are running this script locally, start `tensorboard` and point it to the `logs` directory will allow you to see the loss plot over time. We will not be able to show that easily in Colab environment.\n",
    "\n",
    "You can also continue training by calling the controller again. Try this later and see if the poems generated gets better over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rW284a7xioaw"
   },
   "source": [
    "## Clear previous output\n",
    "\n",
    "Tensorflow will automatically load previous models if you specify a path for the `session`. However, that will be a problem if you change some parts of the model. e.g., change embedding size, LSTM size, or number of layers.\n",
    "\n",
    "You will see something like \n",
    "```\n",
    "INFO:tensorflow:Restoring parameters from logs/model.ckpt-4465\n",
    "...\n",
    "InvalidArgumentError: Assign requires shapes of both tensors to match.\n",
    "```\n",
    "Always remember to clear output directory if you are experimenting with different model structures!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dfkNbkgyG81Z"
   },
   "outputs": [],
   "source": [
    "!rm -R logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XWfT6DruhUBd"
   },
   "source": [
    "# Summary\n",
    "What we learned today:\n",
    "1. Preprocessing for language modeling data\n",
    "    * Create a dictionary that maps words to unique IDs\n",
    "    * Convert words to ID\n",
    "    * Reshape sequences to unified lengths\n",
    "    * Create a helper to produce data\n",
    "2. Building a model using tensorflow\n",
    "    * Hyperparameters\n",
    "    * Training operation\n",
    "    * Testing operation\n",
    "    * Control function\n",
    "3. Training and evaluation\n",
    "    * Observe loss\n",
    "    * Evaluate on test set\n",
    "\n",
    "You are now capable of building a deep learning model for a basic seq2seq task using **tensorflow**! \n",
    "\n",
    "However, tensorflow is extremely complicated (but powerful). There are numerous examples online for you to explore.\n",
    "\n",
    "## Extension\n",
    "\n",
    "Can you think of anything else that may also be learned using this model? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TDKza528g_pK"
   },
   "source": [
    "# Appendix: connect your Google Drive to Colab for uploading your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Z3N08zCdhJG"
   },
   "source": [
    "First, copy the file into Google Drive. Then run the following code to link your Drive to this notebook. Follow the link in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NI3J6rPSqRHP"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MbRchpOzdgm2"
   },
   "source": [
    "Copy (`cp`) the file from `/gdrive` to this server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S-ObQ860GsUw"
   },
   "outputs": [],
   "source": [
    "!cp /gdrive/My\\ Drive/Colab\\ Notebooks/poetry.txt ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EY_0LsbNK42a"
   },
   "outputs": [],
   "source": [
    "!cp /gdrive/My\\ Drive/Colab\\ Notebooks/Book*.txt ./"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Q4i6EI9KAy9"
   },
   "source": [
    "# Part III: Attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VAQQu-1jKNI0"
   },
   "source": [
    "## Task: Translation\n",
    "Hints on how to add attention in seq2seq model in order to perform translation. \n",
    "### CWMT corpus\n",
    "This is a Chinese-English translation dataset.\n",
    "\n",
    "Visit source website to download manually:\n",
    "http://nlp.nju.edu.cn/cwmt-wmt/\n",
    "\n",
    "Take a look at some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BW-EFHSpdqQu"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.cudnn_rnn import CudnnLSTM\n",
    "from tensorflow.contrib.rnn import BasicLSTMCell, MultiRNNCell\n",
    "from tensorflow.nn import embedding_lookup, dropout\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s-E6MEjLb-kV"
   },
   "outputs": [],
   "source": [
    "c_sents = [ss.strip() for ss in open('Book14_cn.txt').readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vYVvCJ2OcO78"
   },
   "outputs": [],
   "source": [
    "c_sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RD0_RQvZcXnV"
   },
   "outputs": [],
   "source": [
    "e_sents=[ss.strip() for ss in open('Book14_en.txt').readlines()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1q3yPC_3cf6Y"
   },
   "outputs": [],
   "source": [
    "e_sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ojo9ek5Zchvt"
   },
   "outputs": [],
   "source": [
    "c_tokenizer = Tokenizer(num_words=None, lower=False, char_level=True)\n",
    "# Create word to ID dictionary\n",
    "c_tokenizer.fit_on_texts(c_sents)\n",
    "# Get dictionary\n",
    "c_word_index = c_tokenizer.word_index\n",
    "# Fix word to ID\n",
    "c_word_index = {c:i+1 for c, i in c_word_index.items()}\n",
    "c_word_index[\"<PAD>\"] = 0\n",
    "c_word_index[\"<UNK>\"] = 1\n",
    "c_tokenizer.word_index = c_word_index\n",
    "c_reverse_word_index = dict([(v, k) for (k, v) in c_word_index.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tiWR9KS1c8rP"
   },
   "outputs": [],
   "source": [
    "# sort word index by ID\n",
    "for (w,i) in sorted(c_word_index.items(), key=lambda w: w[1]):\n",
    "# print some words to check if there are errors!\n",
    "  if i > 10 and i < len(c_word_index)-5: continue\n",
    "  print(\"{} {}\".format(w,i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QPWpPpDZdFBv"
   },
   "outputs": [],
   "source": [
    "e_vocab_size = 20000\n",
    "e_tokenizer = Tokenizer(num_words=e_vocab_size, lower=True, oov_token=\"<UNK>\")\n",
    "# Create word to ID dictionary\n",
    "e_tokenizer.fit_on_texts(e_sents)\n",
    "# Get dictionary\n",
    "e_word_index = e_tokenizer.word_index\n",
    "# Fix word to ID\n",
    "e_word_index = {e:i+1 for e, i in e_word_index.items() if i < e_vocab_size-1}\n",
    "e_word_index[\"<PAD>\"] = 0\n",
    "e_word_index[\"<UNK>\"] = 1\n",
    "e_tokenizer.word_index = e_word_index\n",
    "e_reverse_word_index = dict([(v, k) for (k, v) in e_word_index.items()])\n",
    "# sort word index by ID\n",
    "for (w,i) in sorted(e_word_index.items(), key=lambda w: w[1]):\n",
    "# print some words to check if there are errors!\n",
    "  if i > 10 and i < len(e_word_index)-5: continue\n",
    "  print(\"{} {}\".format(w,i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w_Fp-cEgRaEm"
   },
   "outputs": [],
   "source": [
    "c_sents = c_tokenizer.texts_to_sequences(c_sents)\n",
    "e_sents = e_tokenizer.texts_to_sequences(e_sents)\n",
    "c_sents = pad_sequences(c_sents,value=c_word_index[\"<PAD>\"], padding='post', truncating='post', maxlen=10)\n",
    "e_sents = pad_sequences(e_sents,value=e_word_index[\"<PAD>\"], padding='post', truncating='post', maxlen=10)\n",
    "train_data = (c_sents[:-5], e_sents[:-5])\n",
    "test_data = (c_sents[-5:], e_sents[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gOSdLW6iSgUi"
   },
   "outputs": [],
   "source": [
    "train_data[0][0], train_data[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WvZG4nM807FB"
   },
   "source": [
    "### Change hyperparameters\n",
    "* Add separate vocabulary sized for English and Chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "77XNXnxlOeN-"
   },
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "class Hparam(object):\n",
    "  # ...\n",
    "  source_vocab_size = len(c_word_index)\n",
    "  target_vocab_size = len(e_word_index)\n",
    "  # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F-oIyT8q2zKw"
   },
   "source": [
    "### Prepare input for translation\n",
    "\n",
    "* Modify `class PoemInput(object)` to create different source and target sentences. Most importantly, change the final part.\n",
    "* Use `pad_sequences` to pad both Chinese and English sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y6vD46nKN3Wi"
   },
   "outputs": [],
   "source": [
    "class TranslationInput(object):\n",
    "  def __init__(self, data, config, name=None):\n",
    "    self.batch_size = batch_size = config.batch_size\n",
    "    self.num_steps = config.num_steps\n",
    "    self.sources, self.targets = self.input_producer(\n",
    "        data, batch_size, name=name)\n",
    "\n",
    "  def input_producer(self, raw_data, batch_size, name=None):\n",
    "    source_data = tf.convert_to_tensor(raw_data[0], name=\"source_data\", dtype=tf.int32)\n",
    "    target_data = tf.convert_to_tensor(raw_data[1], name=\"target_data\", dtype=tf.int32)\n",
    "\n",
    "    num_batches = len(raw_data[0]) // self.batch_size\n",
    "    i = tf.train.range_input_producer(num_batches, shuffle=False).dequeue()\n",
    "    x = source_data[i*self.batch_size:(i+1)*self.batch_size, :]\n",
    "    y = target_data[i*self.batch_size:(i+1)*self.batch_size, :]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aZse9WetxH96"
   },
   "source": [
    "## Build Translation Model\n",
    "This is also a seq2seq model, with some major differences:\n",
    "* Use one LSTM as the encoder\n",
    "* Add another as the decoder\n",
    "\n",
    "```python\n",
    "def _build_rnn_encoder\n",
    "    # RNN requires time-major\n",
    "    inputs = tf.transpose(inputs, [1, 0, 2])\n",
    "    self._enccell = CudnnLSTM(\n",
    "        num_layers=config.num_layers,\n",
    "        num_units=config.hidden_size,\n",
    "        name=name)\n",
    "    outputs, state = self._enccell(inputs)\n",
    "    return outputs, state\n",
    "\n",
    "def _build_rnn_decoder\n",
    "    self._deccell = CudnnLSTM(\n",
    "            num_layers=config.num_layers,\n",
    "            num_units=config.hidden_size,\n",
    "            name=name)\n",
    "\n",
    "    outputs, state = self._deccell(inputs)\n",
    "    # Transpose from time-major to batch-major\n",
    "    outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "    return outputs, state\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pHL4dviMyCG8"
   },
   "source": [
    "### Modify training controller\n",
    "\n",
    "Use `TranslationInput` and `MyTranslationModel`.\n",
    "\n",
    "```python\n",
    "train_input = TranslationInput(train_data, config, name=\"TrainInput\")\n",
    "\n",
    "m = MyTranslationModel(is_training=True, config=config, input_=train_input)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SW46OsxK2OTD"
   },
   "source": [
    "### Snippet for adding attention mechanism in the model\n",
    "* Calculate attention score\n",
    "* Normalize score\n",
    "* Calculate context vector = *attention weighted sum*\n",
    "* Concatenate context vector with input\n",
    "* Use decoder to decode next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y7aIxZfaTyDb"
   },
   "outputs": [],
   "source": [
    "# Require:\n",
    "# hidden: decoder hidden (memory)\n",
    "# enc_output: encoder output\n",
    "        \n",
    "# hidden shape == (batch_size, hidden size)\n",
    "# hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "# we are doing this to perform addition to calculate the score\n",
    "hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "# enc_output shape == (batch_size, max_length, hidden_size)\n",
    "# score shape == (batch_size, max_length, hidden_size)\n",
    "score = tf.nn.tanh(W1(enc_output) + W2(hidden_with_time_axis))\n",
    "\n",
    "# attention_weights shape == (batch_size, max_length, 1)\n",
    "# we get 1 at the last axis because we are applying score to V\n",
    "attention_weights = tf.nn.softmax(V(score), axis=1)\n",
    "\n",
    "# context_vector shape after sum == (batch_size, hidden_size)\n",
    "context_vector = attention_weights * enc_output\n",
    "context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "# x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "x = self.embedding(input_)\n",
    "\n",
    "# x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "# passing the concatenated vector to the decoder\n",
    "output, state = self.decoder(x)\n",
    "\n",
    "# output shape == (batch_size * 1, hidden_size)\n",
    "output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "# output shape == (batch_size * 1, vocab)\n",
    "x = SoftmaxLayer(output)\n",
    "\n",
    "# Output:\n",
    "# x: decoder output\n",
    "# state: decoder state\n",
    "# attention_weights: weights over encoder output at one time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ue2W5_FFbZze"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Week11_NLP_codelab_Seq2seq",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
